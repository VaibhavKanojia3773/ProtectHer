{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1284: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvNamedWindow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Run the function to process the camera feed\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[43mprocess_video_from_camera\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m, in \u001b[0;36mprocess_video_from_camera\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Use camera index 0 for the default camera\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Set the window to fullscreen\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamedWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVideo Feed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWINDOW_NORMAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m cv2\u001b[38;5;241m.\u001b[39msetWindowProperty(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo Feed\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mWND_PROP_FULLSCREEN, cv2\u001b[38;5;241m.\u001b[39mWINDOW_FULLSCREEN)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1284: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvNamedWindow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Load models\n",
    "harassment_model_path = \"harass.h5\"\n",
    "harassment_model = tf.keras.models.load_model(harassment_model_path)\n",
    "\n",
    "gender_model_path = \"gender_classification_model.h5\"\n",
    "gender_model = tf.keras.models.load_model(gender_model_path)\n",
    "\n",
    "expression_model_path = \"emotion_model.h5\"\n",
    "expression_model = tf.keras.models.load_model(expression_model_path)\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def process_frame_for_harassment(frame):\n",
    "    temp_img_path = \"temp_frame.jpg\"\n",
    "    cv2.imwrite(temp_img_path, frame)\n",
    "    img = load_img(temp_img_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    features = base_model.predict(img_array)\n",
    "    features = features.reshape(1, 7 * 7 * 512)  # This may need adjusting\n",
    "    predictions = harassment_model.predict(features)\n",
    "    return np.argmax(predictions[0])\n",
    "\n",
    "def process_frame_for_gender(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "    male_count = 0\n",
    "    female_count = 0\n",
    "    lone_woman_detected = False\n",
    "    detected_emotions = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        margin = int(max(w, h) * 0.3)\n",
    "        x1, y1 = max(0, x - margin), max(0, y - margin)\n",
    "        x2, y2 = min(frame.shape[1], x + w + margin), min(frame.shape[0], y + h + margin)\n",
    "        \n",
    "        roi = frame[y1:y2, x1:x2]\n",
    "        roi_resized = cv2.resize(roi, (64, 64))\n",
    "        roi_array = img_to_array(roi_resized) / 255.0\n",
    "        roi_array = np.expand_dims(roi_array, axis=0)\n",
    "        \n",
    "        prediction = gender_model.predict(roi_array)\n",
    "        result = \"Male\" if prediction[0][0] >= 0.5 else \"Female\"\n",
    "        \n",
    "        if result == \"Male\":\n",
    "            male_count += 1\n",
    "        else:\n",
    "            female_count += 1\n",
    "            if male_count == 0:\n",
    "                lone_woman_detected = True\n",
    "        \n",
    "        # Draw rectangle around the face\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Gender label above the bounding box\n",
    "        label = f\"{result} ({prediction[0][0]:.2f})\"\n",
    "        label_position = (x1, y1 - 10) if y1 - 10 > 10 else (x1, y2 + 30)\n",
    "        cv2.putText(frame, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame, male_count, female_count, lone_woman_detected, detected_emotions\n",
    "\n",
    "def process_frame_for_emotion(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    detected_emotions = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        margin = int(max(w, h) * 0.3)\n",
    "        x1, y1 = max(0, x - margin), max(0, y - margin)\n",
    "        x2, y2 = min(frame.shape[1], x + w + margin), min(frame.shape[0], y + h + margin)\n",
    "        \n",
    "        face_roi = gray[y1:y2, x1:x2]\n",
    "        face_roi_resized = cv2.resize(face_roi, (48, 48))\n",
    "        face_roi_normalized = face_roi_resized / 255.0\n",
    "        face_roi_normalized = face_roi_normalized.astype(np.float32)\n",
    "        face_roi_rgb = cv2.cvtColor(face_roi_normalized, cv2.COLOR_GRAY2RGB)\n",
    "        face_roi_expanded = np.expand_dims(face_roi_rgb, axis=0)\n",
    "        \n",
    "        try:\n",
    "            emotion_prediction = expression_model.predict(face_roi_expanded)\n",
    "            emotion_label = emotion_labels[np.argmax(emotion_prediction)]\n",
    "            detected_emotions.append(emotion_label)\n",
    "            \n",
    "            # Emotion label below the bounding box\n",
    "            label_position = (x1, y2 + 30) if y2 + 30 < frame.shape[0] else (x1, y1 - 10)\n",
    "            cv2.putText(frame, emotion_label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        except Exception as e:\n",
    "            print(\"Error during emotion prediction:\", str(e))\n",
    "    \n",
    "    return frame, detected_emotions\n",
    "\n",
    "def process_video_from_camera():\n",
    "    cap = cv2.VideoCapture(0)  # Use camera index 0 for the default camera\n",
    "\n",
    "    # Set the window to fullscreen\n",
    "    cv2.namedWindow('Video Feed', cv2.WINDOW_NORMAL)\n",
    "    cv2.setWindowProperty('Video Feed', cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        harassment_prediction = process_frame_for_harassment(frame)\n",
    "        frame, male_count, female_count, lone_woman_detected, detected_emotions = process_frame_for_gender(frame)\n",
    "        frame, detected_emotions = process_frame_for_emotion(frame)\n",
    "        \n",
    "        # Check for harassment based on detected emotions\n",
    "        emotion_criteria = ['Disgust', 'Angry', 'Fear']\n",
    "        harassment_detected = any(emotion in detected_emotions for emotion in emotion_criteria)\n",
    "        \n",
    "        # Updated Criteria for Harassment Detection\n",
    "        if male_count > 0 and female_count > 0 and harassment_prediction == 1:\n",
    "            if harassment_detected:\n",
    "                cv2.putText(frame, \"!!! HARASSMENT DETECTED !!!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 4)\n",
    "        \n",
    "        if lone_woman_detected and male_count == 0:\n",
    "            cv2.putText(frame, \"!!! LONE WOMAN DETECTED !!!\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 4)\n",
    "        \n",
    "        if lone_woman_detected and male_count > 0:\n",
    "            cv2.putText(frame, f\"LONE WOMAN SURROUNDED BY {male_count} MEN\", (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 4)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Video Feed', frame)\n",
    "\n",
    "        # Exit condition\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the function to process the camera feed\n",
    "process_video_from_camera()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
